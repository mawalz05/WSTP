{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WSTP_NLPModel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOhRqPsajmfqb66wsbK8TB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mawalz05/WSTP/blob/main/WSTP_NLPModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAHUNOwT12c_"
      },
      "source": [
        "# Installng tensorflow and transfomers to use pre-trained BERT\n",
        "import tensorflow as tf\n",
        "!pip install -q transformers\n",
        "\n",
        "# Importing the data from local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "uploaded2 = files.upload()\n",
        "uploaded3 = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8vRp7zH18Pf"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df_1 = pd.read_csv(io.BytesIO(uploaded['nlp_batch_3_complete.csv']), encoding = \"ISO-8859-1\")\n",
        "#df2 = pd.read_csv(io.BytesIO(uploaded2['NLP_test.csv']), encoding = \"ISO-8859-1\") # This is the new data for prediction\n",
        "\n",
        "df_1 = df_1[['text', 'sustainable']]\n",
        "\n",
        "# This is in case of missing values\n",
        "df_1 = df_1.dropna()\n",
        "\n",
        "# Merging Datasets\n",
        "df_2 = pd.read_csv(io.BytesIO(uploaded2['nlp_batch_csr_complete.csv']), encoding = \"ISO-8859-1\")\n",
        "df_2 = df_2[['text','sustainable']]\n",
        "df_2 = df_2.dropna()\n",
        "\n",
        "df = pd.concat([df_1, df_2], axis = 0)\n",
        "\n",
        "\n",
        "# Turning the label column into binary\n",
        "df['label'] = pd.np.where(df['sustainable'] == 'Company', 1, df['sustainable'])\n",
        "df['label'] = pd.np.where(df['sustainable'] == 'General', 1, df['label'])\n",
        "df['label'] = np.where(df['sustainable'] == 'No', 0, df['label'])\n",
        "\n",
        "df = df[(df['label'] == 0) | (df['label'] == 1)]\n",
        "\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "# Shuffling the data and creating a test sample to run quicker if we want\n",
        "# df_test = df.sample(frac = 1)\n",
        "# df_test = df_test[:100000]\n",
        "\n",
        "# Creating target and feature vectors\n",
        "# target = df_test['y']\n",
        "# features = df_test['text']\n",
        "target = df['label']\n",
        "features = df['text']\n",
        "\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = .30, shuffle = True)\n",
        "\n",
        "# Turning the feature text into a lists of tweets\n",
        "text_list_train = X_train.tolist()\n",
        "text_list_test = X_test.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ac0bYDvVYLBH"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIzZq4U71-_S"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "\n",
        "# max length can be up to 512 for BERT\n",
        "max_length = 50\n",
        "\n",
        "# The recommended batch size for BERT is 16, 32, ... smaller batches work better for regularization\n",
        "batch_size = 6\n",
        "\n",
        "# Creating a function that tokenizes the data\n",
        "def convert_example_to_feature(tweet):\n",
        "  return tokenizer.encode_plus(\n",
        "      tweet, add_special_tokens = True, # add [CLS] and [SEP]\n",
        "      max_length = max_length, # max length of the text that can go to BERT\n",
        "      pad_to_max_length = True, # Add [PAD] to tokens\n",
        "      return_attention_mask = True, #Add attention mask to not focuson on pad tokens\n",
        "      )\n",
        "\n",
        "# Creating a function that can map the input, tokens, and attention masks, and the label\n",
        "def map_example_to_dict(input_ids, attention_mask, token_type_ids, label):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "  }, label\n",
        "\n",
        "# Creating a function to conform to the input standards of BERT\n",
        "def encode_examples(X, y, limit = -1):\n",
        "  input_ids_list = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "  label_list = []\n",
        "\n",
        "  if (limit > 0):\n",
        "    X = X.take(limit)\n",
        "    y = y.take(limit)\n",
        "\n",
        "  for tweet in X:\n",
        "    bert_input = convert_example_to_feature(tweet)\n",
        "\n",
        "    input_ids_list.append(bert_input['input_ids'])\n",
        "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "    attention_mask_list.append(bert_input['attention_mask'])\n",
        "  \n",
        "  for label in y:\n",
        "    label_list.append([tf.dtypes.cast(label, tf.int32)])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWNpfy_X6rjl"
      },
      "source": [
        "# Conforming the training data to BERT standards\n",
        "ds_train_encoded = encode_examples(text_list_train, y_train).shuffle(10000).batch(batch_size)\n",
        "\n",
        "# Conforming the testing data to BERT standards\n",
        "ds_test_encoded = encode_examples(text_list_test, y_test).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRFMlXMw2A8g"
      },
      "source": [
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Recommended learning rates for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Running only five epochs for illustration\n",
        "number_of_epochs = 5\n",
        "\n",
        "# Model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Use Adam\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, epsilon = 1e-08)\n",
        "\n",
        "# We do not have one-hot vectors, so we can use sparse categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QJUSckA2EEk"
      },
      "source": [
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Now we can begin fine tuning\n",
        "bert_history = model.fit(ds_train_encoded, epochs = number_of_epochs, validation_data = ds_test_encoded, callbacks=[cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpoxnZs8Ik7I"
      },
      "source": [
        "df2 = pd.read_csv(io.BytesIO(uploaded3['nlp_unlabelled_test.csv']), encoding = \"ISO-8859-1\") # This is the new data for prediction\n",
        "\n",
        "# df2['label'] = pd.np.where(df2['sustainable'] == 'yes', 1, df2['Sustainable'])\n",
        "# df2['label'] = np.where(df2['label'] == 'no', 0, df2['label'])\n",
        "\n",
        "features_new = df2['text']\n",
        "target_new = df2['sustainable']\n",
        "\n",
        "text_list_new = features_new.tolist()\n",
        "\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "\n",
        "# max length can be up to 512 for BERT\n",
        "max_length = 50\n",
        "\n",
        "# The recommended batch size for BERT is 16, 32, ... smaller batches work better for regularization\n",
        "batch_size = 6\n",
        "\n",
        "# Creating a function that tokenizes the data\n",
        "def convert_example_to_feature(tweet):\n",
        "  return tokenizer.encode_plus(\n",
        "      tweet, add_special_tokens = True, # add [CLS] and [SEP]\n",
        "      max_length = max_length, # max length of the text that can go to BERT\n",
        "      pad_to_max_length = True, # Add [PAD] to tokens\n",
        "      return_attention_mask = True, #Add attention mask to not focuson on pad tokens\n",
        "      )\n",
        "\n",
        "# Creating a function that can map the input, tokens, and attention masks, and the label\n",
        "def map_example_to_dict(input_ids, attention_mask, token_type_ids):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"token_type_ids\": token_type_ids,\n",
        "      \"attention_mask\": attention_mask,\n",
        "  }\n",
        "\n",
        "# Creating a function to conform to the input standards of BERT\n",
        "def encode_examples(X, limit = -1):\n",
        "  input_ids_list = []\n",
        "  token_type_ids_list = []\n",
        "  attention_mask_list = []\n",
        "  # label_list = []\n",
        "\n",
        "  if (limit > 0):\n",
        "    X = X.take(limit)\n",
        "    # y = y.take(limit)\n",
        "\n",
        "  for tweet in X:\n",
        "    bert_input = convert_example_to_feature(tweet)\n",
        "\n",
        "    input_ids_list.append(bert_input['input_ids'])\n",
        "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "    attention_mask_list.append(bert_input['attention_mask'])\n",
        "  \n",
        "  # for label in y:\n",
        "  #   label_list.append([tf.dtypes.cast(label, tf.int32)])\n",
        "\n",
        "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list)).map(map_example_to_dict)\n",
        "\n",
        "# Conforming the testing data to BERT standards\n",
        "ds_new_encoded = encode_examples(text_list_new).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjBLoU6SG83T"
      },
      "source": [
        "sentiment = model.predict(ds_new_encoded, verbose = 1)\n",
        "#tf_prediction = tf.nn.sigmoid(sentiment[0], axis = -1)\n",
        "tf_prediction = tf.nn.sigmoid(sentiment[0])\n",
        "label = tf.argmax(tf_prediction, axis = 1).numpy()\n",
        "den = len(label)\n",
        "nom = sum(label)\n",
        "\n",
        "print(den)\n",
        "print(nom)\n",
        "score = nom/den\n",
        "print(score)\n",
        "\n",
        "total = (score/2 + .5)*100\n",
        "print(total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sc0uaUzVeum"
      },
      "source": [
        "######################################################\n",
        "# This is the end"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82-az8xSVeiO"
      },
      "source": [
        "####################################################\n",
        "# Extra code not needed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MqJQrp4-PeS"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_path)\n",
        "\n",
        "from transformers import TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Recommended learning rates for Adam 5e-5, 3e-5, 2e-5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Running only five epochs for illustration\n",
        "number_of_epochs = 5\n",
        "\n",
        "# Model initialization\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Use Adam\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate, epsilon = 1e-08)\n",
        "\n",
        "# We do not have one-hot vectors, so we can use sparse categorical cross entropy and accuracy\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = [metric])\n",
        "\n",
        "model.load_weights(latest)\n",
        "\n",
        "# Make predictions\n",
        "sentiment = model.predict_proba(ds_new_encoded, verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}